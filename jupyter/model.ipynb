{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:24:08.459624Z",
     "start_time": "2020-02-04T16:24:08.456990Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T21:25:45.519807Z",
     "start_time": "2020-02-04T21:25:44.246643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (lat: 1500, lon: 3600, time: 1)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1999-01-04\n",
       "  * lon      (lon) float64 -179.9 -179.8 -179.8 -179.6 ... 179.8 179.9 180.0\n",
       "  * lat      (lat) float64 89.95 89.85 89.75 89.65 ... -59.75 -59.85 -59.95\n",
       "Data variables:\n",
       "    dis24    (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    CDI:                       Climate Data Interface version 1.9.6 (http://m...\n",
       "    Conventions:               CF-1.6\n",
       "    history:                   Thu Oct 10 12:28:52 2019: cdo -seldate,1999-01...\n",
       "    cdo_openmp_thread_number:  8\n",
       "    NCO:                       netCDF Operators version 4.7.8 (Homepage = htt...\n",
       "    CDO:                       Climate Data Operators version 1.9.6 (http://m...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 1500, lon: 3600, time: 1)\n",
       "Coordinates:\n",
       "  * time     (time) datetime64[ns] 1999-01-04\n",
       "  * lon      (lon) float64 -179.9 -179.8 -179.8 -179.6 ... 179.8 179.9 180.0\n",
       "  * lat      (lat) float64 89.95 89.85 89.75 89.65 ... -59.75 -59.85 -59.95\n",
       "Data variables:\n",
       "    dis24    (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    CDI:                       Climate Data Interface version 1.9.6 (http://m...\n",
       "    Conventions:               CF-1.6\n",
       "    history:                   Thu Oct 10 12:28:52 2019: cdo -seldate,1999-01...\n",
       "    cdo_openmp_thread_number:  8\n",
       "    NCO:                       netCDF Operators version 4.7.8 (Homepage = htt...\n",
       "    CDO:                       Climate Data Operators version 1.9.6 (http://m..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr\n",
    "xr.open_dataset('/mnt/bucket/stuarts_files/glofas/1999/CEMS_ECMWF_dis24_19990104_glofas_v2.1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:24:08.762694Z",
     "start_time": "2020-02-04T16:24:08.759447Z"
    }
   },
   "outputs": [],
   "source": [
    "from functions.floodmodel_utils import get_basin_mask, get_river_mask, reshape_scalar_predictand, reshape_multiday_predictand\n",
    "import xarray as xr\n",
    "#Creating a Dask local cluster for parallel computing (making the computations later on much faster)\n",
    "from dask.distributed import Client, LocalCluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:24:09.195910Z",
     "start_time": "2020-02-04T16:24:09.162043Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/main/lib/python3.6/site-packages/distributed/dashboard/core.py:72: UserWarning: \n",
      "Port 8787 is already in use. \n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the diagnostics dashboard on a random port instead.\n",
      "  warnings.warn(\"\\n\" + msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#HYPERPARAMETERS\n",
    "days_intake_length = 90\n",
    "forecast_day = 30\n",
    "\n",
    "\n",
    "#LIBRARY IMPORTS\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "\n",
    "#Connecting to a cluster to be able to run the code locally/on the cloud\n",
    "#cluster = LocalCluster()  # n_workers=10, threads_per_worker=1,\n",
    "client = Client(processes=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-04T16:24:11.784829Z",
     "start_time": "2020-02-04T16:24:11.780516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dashboard': 34803}\n"
     ]
    }
   ],
   "source": [
    "#This will tell you where you dashboard will be so you can visualize your model being run\n",
    "print(client.scheduler_info()['services'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:24:12.741Z"
    }
   },
   "outputs": [],
   "source": [
    "glofas_loaded = xr.open_mfdataset(\"/mnt/bucket/stuarts_files/glofas/*/*.nc\", combine=\"by_coords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:24:13.959Z"
    }
   },
   "outputs": [],
   "source": [
    "era5_loaded = xr.open_mfdataset(\"/mnt/bucket/stuarts_files/Elbe/reanalysis-era5-single-levels_convective_precipitation,land_sea_mask,large_scale_precipitation,runoff,slope_of_sub_gridscale_orography,soil_type,total_column_water_vapour,volumetric_soil_water_layer_1,volumetric_soil_water_layer_2_*.nc\", combine=\"by_coords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:12:21.395Z"
    }
   },
   "outputs": [],
   "source": [
    "glofas = glofas_loaded.copy()\n",
    "era5 = era5_loaded.copy()\n",
    "glofas = glofas.rename({'lon' : 'longitude'})\n",
    "glofas = glofas.rename({'lat': 'latitude'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:12:22.052Z"
    }
   },
   "outputs": [],
   "source": [
    "glofas_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:17:37.262Z"
    }
   },
   "outputs": [],
   "source": [
    "elbe_basin_mask = get_basin_mask(glofas['dis24'].isel(time=0), 'Elbe')\n",
    "elbe_river_mask = get_river_mask(glofas['dis24'].isel(time=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:17:37.649Z"
    }
   },
   "outputs": [],
   "source": [
    "glofas_masked = glofas.where(elbe_basin_mask, drop=True)\n",
    "glofas_river_masked = glofas_masked.where(elbe_river_mask, drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:17:38.376Z"
    }
   },
   "outputs": [],
   "source": [
    "glofas_masked['dis24'].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:17:38.598Z"
    }
   },
   "outputs": [],
   "source": [
    "glofas_river_masked['dis24'].isel(time=0).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-04T16:17:38.772Z"
    }
   },
   "outputs": [],
   "source": [
    "y_orig = glofas_masked['dis24']\n",
    "#Making a copy because y will be transformed to represent the variation of discharge. The model will be predicting the variation of discharge, not the quantity of discharge itself\n",
    "y = y_orig.copy()\n",
    "\n",
    "#Era5 will be the predictor dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-03T04:42:20.893Z"
    }
   },
   "outputs": [],
   "source": [
    "y.to_netcdf(\"/mnt/bucket/stuarts_files/elbedischarge.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:29:07.867639Z",
     "start_time": "2020-02-03T04:29:05.059506Z"
    }
   },
   "outputs": [],
   "source": [
    "era5 = era5.interp(latitude=glofas.latitude, longitude=glofas.longitude).where(elbe_basin_mask, drop=True)\n",
    "X = era5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:38:11.071593Z",
     "start_time": "2020-02-03T04:38:10.998980Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3abed6c0fd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:29:07.981671Z",
     "start_time": "2020-02-03T04:29:07.930800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.DataArray &#x27;dis24&#x27; (time: 7610, latitude: 54, longitude: 71)&gt;\n",
       "dask.array&lt;where, shape=(7610, 54, 71), dtype=float32, chunksize=(1, 54, 71), chunktype=numpy.ndarray&gt;\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float64 9.25 9.35 9.45 9.55 ... 16.05 16.15 16.25\n",
       "  * latitude   (latitude) float64 53.85 53.75 53.65 53.55 ... 48.75 48.65 48.55\n",
       "  * time       (time) datetime64[ns] 1999-01-01 1999-01-02 ... 2019-11-01\n",
       "Attributes:\n",
       "    units:    m3/s</pre>"
      ],
      "text/plain": [
       "<xarray.DataArray 'dis24' (time: 7610, latitude: 54, longitude: 71)>\n",
       "dask.array<where, shape=(7610, 54, 71), dtype=float32, chunksize=(1, 54, 71), chunktype=numpy.ndarray>\n",
       "Coordinates:\n",
       "  * longitude  (longitude) float64 9.25 9.35 9.45 9.55 ... 16.05 16.15 16.25\n",
       "  * latitude   (latitude) float64 53.85 53.75 53.65 53.55 ... 48.75 48.65 48.55\n",
       "  * time       (time) datetime64[ns] 1999-01-01 1999-01-02 ... 2019-11-01\n",
       "Attributes:\n",
       "    units:    m3/s"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:29:09.467800Z",
     "start_time": "2020-02-03T04:29:07.985800Z"
    }
   },
   "outputs": [],
   "source": [
    "X_small = X.mean(['longitude', 'latitude'])\n",
    "y_small = y.mean(['latitude','longitude'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:29:09.546755Z",
     "start_time": "2020-02-03T04:29:09.471033Z"
    }
   },
   "outputs": [],
   "source": [
    "period_train = dict(time=slice(None, '1999-01-15'))\n",
    "\n",
    "\n",
    "X_small, y_small = X_small.loc[period_train], y_small.loc[period_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-03T04:37:08.291010Z",
     "start_time": "2020-02-03T04:36:44.411084Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 190.19 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 191.83 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 193.78 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 195.74 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 197.70 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 199.68 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 201.64 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 203.63 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 205.67 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 207.68 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 209.72 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 211.70 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 213.71 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 214.17 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Worker is at 80% memory usage. Pausing worker.  Process memory: 215.82 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 215.82 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 215.83 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 216.15 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 216.57 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 217.01 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 217.44 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 217.79 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 218.17 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 218.66 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 219.18 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 219.49 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 220.10 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 220.74 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 221.18 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 221.85 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 222.40 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 222.90 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 223.41 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 223.83 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 224.21 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 224.48 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 224.75 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 225.13 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 225.45 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 225.70 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 225.98 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 226.33 GB -- Worker memory limit: 268.11 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 226.65 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 226.94 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 227.25 GB -- Worker memory limit: 268.11 GB\n",
      "distributed.worker - WARNING - Memory use is high but worker has no data to store to disk.  Perhaps some other process is leaking memory?  Process memory: 227.51 GB -- Worker memory limit: 268.11 GB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-50ad71ea8554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_small\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \"\"\"\n\u001b[1;32m    804\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_persist_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m             \u001b[0;31m# evaluate all the dask arrays simultaneously\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0mevaluated_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlazy_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluated_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/dask/base.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_keys__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0mpostcomputes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dask_postcompute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrepack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostcomputes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, dsk, keys, restrictions, loose_restrictions, resources, sync, asynchronous, direct, retries, priority, fifo_timeout, actors, **kwargs)\u001b[0m\n\u001b[1;32m   2574\u001b[0m                     \u001b[0mshould_rejoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2576\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2577\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2578\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36mgather\u001b[0;34m(self, futures, errors, direct, asynchronous)\u001b[0m\n\u001b[1;32m   1872\u001b[0m                 \u001b[0mdirect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdirect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0mlocal_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m                 \u001b[0masynchronous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1875\u001b[0m             )\n\u001b[1;32m   1876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/distributed/client.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             return sync(\n\u001b[0;32m--> 769\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m             )\n\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/site-packages/distributed/utils.py\u001b[0m in \u001b[0;36msync\u001b[0;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/main/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_small.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T05:38:09.390Z"
    }
   },
   "outputs": [],
   "source": [
    "y_small.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T05:38:10.286Z"
    }
   },
   "outputs": [],
   "source": [
    "Xda, yda = reshape_scalar_predictand(X_small, y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T05:38:12.148Z"
    }
   },
   "outputs": [],
   "source": [
    "Xda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T05:38:12.440Z"
    }
   },
   "outputs": [],
   "source": [
    "yda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T05:38:13.701Z"
    }
   },
   "outputs": [],
   "source": [
    "concatenated = xr.concat([Xda, Xda], 'points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-02-02T05:38:13.929Z"
    }
   },
   "outputs": [],
   "source": [
    "concatenated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:49:26.872111Z",
     "start_time": "2020-02-02T03:46:26.563Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(0, len(X.latitude), 10)):\n",
    "\n",
    "    for j in range(0, len(X.longitude), 10):\n",
    "        \n",
    "        if i==0 and j==0:\n",
    "            Xda, yda = reshape_scalar_predictand(X.isel(latitude=i, longitude=j), y.isel(latitude=i, longitude=j))\n",
    "            \n",
    "        else:\n",
    "            XdaNew, ydaNew = reshape_scalar_predictand(X.isel(latitude=i, longitude=j), y.isel(latitude=i, longitude=j))\n",
    "\n",
    "            Xda = xr.concat([Xda, XdaNew], 'points')\n",
    "            yda = xr.concat([yda, ydaNew], 'points')\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:49:26.874058Z",
     "start_time": "2020-02-02T03:46:41.945Z"
    }
   },
   "outputs": [],
   "source": [
    "Xda.to_netcdf('/mnt/bucket/stuarts_files/elbe.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-02T04:49:26.876589Z",
     "start_time": "2020-02-02T03:46:42.269Z"
    }
   },
   "outputs": [],
   "source": [
    "yda.to_netcdf('mnt/bucket/starts_files/elbeglofas.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T04:02:15.702224Z",
     "start_time": "2020-01-31T04:00:11.583Z"
    }
   },
   "outputs": [],
   "source": [
    "period_train = dict(time=slice(None, '2005'))\n",
    "period_valid = dict(time=slice('2006', '2011'))\n",
    "period_test = dict(time=slice('2012', '2016'))\n",
    "\n",
    "\n",
    "X_train, y_train = Xda.loc[period_train], yda.loc[period_train]\n",
    "X_valid, y_valid = Xda.loc[period_valid], yda.loc[period_valid]\n",
    "X_test, y_test = Xda.loc[period_test], yda.loc[period_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-31T04:34:35.893473Z",
     "start_time": "2020-01-31T04:34:35.870017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>&lt;xarray.Dataset&gt;\n",
       "Dimensions:    (latitude: 54, longitude: 71, time: 180367)\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 53.85 53.75 53.65 53.55 ... 48.75 48.65 48.55\n",
       "  * longitude  (longitude) float64 9.25 9.35 9.45 9.55 ... 16.05 16.15 16.25\n",
       "  * time       (time) datetime64[ns] 1999-01-01 ... 2019-11-29T06:00:00\n",
       "Data variables:\n",
       "    cp         (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    lsm        (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    lsp        (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    ro         (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    slor       (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    slt        (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    tcwv       (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    swvl1      (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "    swvl2      (time, latitude, longitude) float32 dask.array&lt;chunksize=(744, 54, 71), meta=np.ndarray&gt;\n",
       "Attributes:\n",
       "    Conventions:  CF-1.6\n",
       "    history:      2019-12-02 16:58:32 GMT by grib_to_netcdf-2.14.0: /opt/ecmw...</pre>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:    (latitude: 54, longitude: 71, time: 180367)\n",
       "Coordinates:\n",
       "  * latitude   (latitude) float64 53.85 53.75 53.65 53.55 ... 48.75 48.65 48.55\n",
       "  * longitude  (longitude) float64 9.25 9.35 9.45 9.55 ... 16.05 16.15 16.25\n",
       "  * time       (time) datetime64[ns] 1999-01-01 ... 2019-11-29T06:00:00\n",
       "Data variables:\n",
       "    cp         (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    lsm        (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    lsp        (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    ro         (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    slor       (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    slt        (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    tcwv       (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    swvl1      (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "    swvl2      (time, latitude, longitude) float32 dask.array<chunksize=(744, 54, 71), meta=np.ndarray>\n",
       "Attributes:\n",
       "    Conventions:  CF-1.6\n",
       "    history:      2019-12-02 16:58:32 GMT by grib_to_netcdf-2.14.0: /opt/ecmw..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "y_train_scaled = y_train\n",
    "#I'm hesitating whether I should apply standard scaling on the output\n",
    "# sc2 = MinMaxScaler(feature_range=(0,1))\n",
    "# y_train_scaled = sc2.fit_transform(y_train.values.reshape(-1,1))\n",
    "X_train = []\n",
    "y_train_array = []\n",
    "\n",
    "coordinate_array = []\n",
    "\n",
    "#tqdm, itertools.product.\n",
    "#Iterating through each feature, shifting the time for each feature, and appending the time-shifted feature array to X_train for a total of 16 times\n",
    "for n in range(len(X_train.data_vars)):\n",
    "\n",
    "    feature_array = []\n",
    "    #For every latitude coordinate:\n",
    "    for j in range(len(X_train.latitude)):\n",
    "\n",
    "        for k in range(len(X_train.longitude)):\n",
    "\n",
    "            for l in range(60, len(X_train_scaled)):\n",
    "\n",
    "                feature_array.append(X_train.isel(latitude=j, longitude=k)[l - days_intake_length:l, n])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            coordinate_array.append(feature_array)\n",
    "            feature_array = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    X_train.append(coordinate_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
